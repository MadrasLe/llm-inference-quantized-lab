{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes -q\n",
        "!pip install accelerate -q\n",
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "9cYUlj2FnOvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-98I_j2ImF0C"
      },
      "outputs": [],
      "source": [
        "# =======================================================================================\n",
        "#               INFERENCE SCRIPT FOR ERNIE-4.5-21B-A3B-Thinking (QUANTIZED)\n",
        "#                          Adapted for Baidu's Model\n",
        "# =======================================================================================\n",
        "\n",
        "# @title Step 1: Install Dependencies\n",
        "# Installs required libraries for 4-bit quantization and inference.\n",
        "print(\" Phase 1: Installing dependencies... Please wait. â³\")\n",
        "!pip install transformers bitsandbytes accelerate -q\n",
        "print(\"Installation complete.\\n\")\n",
        "\n",
        "\n",
        "# @title Step 2: Load Model and Tokenizer\n",
        "# Loads the ERNIE model quantized in 4-bit.\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import time\n",
        "\n",
        "print(\" Phase 2: Loading ERNIE-4.5-21B-A3B-Thinking... Please wait.\")\n",
        "\n",
        "# Model Configuration\n",
        "model_name = \"baidu/ERNIE-4.5-21B-A3B-Thinking\"\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load Quantized Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "print(\"\\nModel and Tokenizer loaded successfully.\\n\")\n",
        "\n",
        "\n",
        "# @title Step 3: Prepare Prompt\n",
        "# Setting up the initial text using the ERNIE format.\n",
        "\n",
        "print(\" Phase 3: Preparing the prompt...\")\n",
        "\n",
        "# Chat template for ERNIE\n",
        "# Note: System prompt changed to a standard professional instruction.\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain the Unsloth library to me as if I were 5 years old.\"}\n",
        "]\n",
        "\n",
        "# Apply chat template\n",
        "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Prompt formatted and ready.\\n\")\n",
        "\n",
        "\n",
        "# @title Step 4: Manual Inference with Streaming\n",
        "# Generates text token by token and streams the output.\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"Generating response...\\n\")\n",
        "\n",
        "# -- Start Measurement --\n",
        "start_time = time.time()\n",
        "\n",
        "generated_tokens = 0\n",
        "max_new_tokens = 256\n",
        "\n",
        "# Tokenize initial prompt\n",
        "inputs = tokenizer([formatted_prompt], return_tensors=\"pt\").to(model.device)\n",
        "input_ids_tensor = inputs.input_ids\n",
        "\n",
        "# Store initial prompt length\n",
        "prompt_token_length = input_ids_tensor.shape[1]\n",
        "\n",
        "# Variable to store already printed text to avoid repetition\n",
        "already_decoded_text = \"\"\n",
        "\n",
        "# Use 'torch.no_grad()' to save memory and speed up inference\n",
        "with torch.no_grad():\n",
        "    for _ in range(max_new_tokens):\n",
        "        # 1. Pass tokens to the model\n",
        "        outputs = model(input_ids=input_ids_tensor)\n",
        "\n",
        "        # 2. Get the most probable next token\n",
        "        next_token_logits = outputs.logits[:, -1, :]\n",
        "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "        # 3. Append new token to sequence\n",
        "        input_ids_tensor = torch.cat([input_ids_tensor, next_token_id], dim=-1)\n",
        "\n",
        "        # 4. Increment token count\n",
        "        generated_tokens += 1\n",
        "\n",
        "        # 5. Decode generated part so far\n",
        "        tokens_generated_only = input_ids_tensor[0][prompt_token_length:]\n",
        "        full_decoded_text = tokenizer.decode(tokens_generated_only, skip_special_tokens=True)\n",
        "\n",
        "        # Print only the NEW part of the text\n",
        "        new_text_chunk = full_decoded_text[len(already_decoded_text):]\n",
        "        print(new_text_chunk, end='', flush=True)\n",
        "\n",
        "        # Update printed text reference\n",
        "        already_decoded_text = full_decoded_text\n",
        "\n",
        "        # 6. Stop if model generates EOS token\n",
        "        if next_token_id.item() == eos_token_id:\n",
        "            break\n",
        "\n",
        "# -- End Measurement --\n",
        "end_time = time.time()\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"Generation complete.\")\n",
        "\n",
        "# --- Calculate and display results ---\n",
        "total_time = end_time - start_time\n",
        "tokens_per_second = (generated_tokens / total_time) if total_time > 0 else 0\n",
        "\n",
        "print(f\"\\n **Performance Metrics** \\n\")\n",
        "print(f\" Tokens generated: {generated_tokens}\")\n",
        "print(f\" Total generation time: {total_time:.2f} seconds\")\n",
        "print(f\" **Speed: {tokens_per_second:.2f} tokens per second**\")"
      ]
    }
  ]
}